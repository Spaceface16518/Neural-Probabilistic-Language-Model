{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implementation of the 2003 paper on Neural Probabilistic Language Models\n",
    "by Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Jauvin"
   ],
   "id": "64a1fc40dcf8b80a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:09.659277Z",
     "start_time": "2024-05-21T08:28:09.653711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch"
   ],
   "id": "4a25ce7045e38c0b",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:09.699104Z",
     "start_time": "2024-05-21T08:28:09.690037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the seed for reproducibility\n",
    "# import random\n",
    "\n",
    "# seed = 42\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "22904de4dbabae8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:09.708696Z",
     "start_time": "2024-05-21T08:28:09.702475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')"
   ],
   "id": "2916ad28820a8e33",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:14.799125Z",
     "start_time": "2024-05-21T08:28:09.710309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# prepare the data\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# build vocabulary\n",
    "def build_vocab(corpus):\n",
    "    vocab = Counter(brown.words())\n",
    "\n",
    "    # Consolidate words rare words into a single token\n",
    "    for word in list(vocab.keys()):\n",
    "        if vocab[word] <= 3:\n",
    "            vocab['<unk>'] += vocab[word]\n",
    "            del vocab[word]\n",
    "    \n",
    "    # Create indexes\n",
    "    index_to_word = list(vocab.keys())\n",
    "    word_to_index = {word: i for i, word in enumerate(index_to_word)}\n",
    "\n",
    "    # Convert words to indexes\n",
    "    vocab_indices = [word_to_index[word] for word in brown.words() if word in word_to_index]\n",
    "\n",
    "    return vocab, vocab_indices\n",
    "\n",
    "\n",
    "vocab, vocab_indices = build_vocab(brown.words())\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "\n",
    "# Chunk the data into sequences, X being the first n-1 words and y being the nth word\n",
    "def chunk_data(data, window_size=6):\n",
    "    X = [data[i:i + window_size] for i in range(len(data) - window_size)]\n",
    "    y = [data[i] for i in range(window_size, len(data))]\n",
    "    return tensor(X, device=device), tensor(y, device=device)\n",
    "\n",
    "\n",
    "def get_data(indices, window_size=6, train_samples=800000, test_samples=200000, batch_size=32, shuffle=True):\n",
    "    train_indices = indices[:train_samples]\n",
    "    test_indices = indices[train_samples:train_samples + test_samples]\n",
    "    val_indices = indices[train_samples + test_samples:]\n",
    "\n",
    "    train_X, train_y = chunk_data(train_indices, window_size)\n",
    "    test_X, test_y = chunk_data(test_indices, window_size)\n",
    "    val_X, val_y = chunk_data(val_indices, window_size)\n",
    "\n",
    "    train_dl = DataLoader(TensorDataset(train_X, train_y), batch_size, shuffle)\n",
    "    test_dl = DataLoader(TensorDataset(test_X, test_y), batch_size, shuffle)\n",
    "    val_dl = DataLoader(TensorDataset(val_X, val_y), batch_size, shuffle)\n",
    "\n",
    "    return train_dl, test_dl, val_dl"
   ],
   "id": "b0986e3836280f8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 17905\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:14.807620Z",
     "start_time": "2024-05-21T08:28:14.801972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Neural Probabilistic Language Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size):\n",
    "        super(NPLM, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden = nn.Linear(embedding_dim * window_size, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.tanh(self.hidden(x))  # TODO: implement direct connections (Wx from paper)\n",
    "        x = F.log_softmax(self.output(x), dim=1)\n",
    "        return x"
   ],
   "id": "c184122892b27ccc",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:32:42.670616Z",
     "start_time": "2024-05-21T08:32:42.664264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 60\n",
    "hidden_dim = 100\n",
    "window_size = 6\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Set the trainjng parameters\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64"
   ],
   "id": "e463abc213bb4c6f",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:28:19.317743Z",
     "start_time": "2024-05-21T08:28:14.814256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the data\n",
    "train_dl, test_dl, val_dl = get_data(vocab_indices, batch_size=batch_size)"
   ],
   "id": "3bf7d0926f09eb64",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:38:33.079435Z",
     "start_time": "2024-05-21T08:32:46.017323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = NPLM(vocab_size, embedding_dim, hidden_dim, window_size).to(device)\n",
    "print(model)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "def train_loop(model, train_dl, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X, y in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def eval_loop(model, eval_dl, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for X, y in eval_dl:\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(eval_dl)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_dl, eval_dl, optimizer, criterion):\n",
    "    train_loop(model, train_dl, optimizer, criterion)\n",
    "    loss = eval_loop(model, eval_dl, criterion)\n",
    "    return loss\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    loss = train_one_epoch(model, train_dl, test_dl, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} Loss: {loss}\")\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        torch.save(model.state_dict(), \"nplm_brown.pt\")"
   ],
   "id": "7deac552099f8363",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPLM(\n",
      "  (embeddings): Embedding(17905, 60)\n",
      "  (hidden): Linear(in_features=360, out_features=100, bias=True)\n",
      "  (output): Linear(in_features=100, out_features=17905, bias=True)\n",
      ")\n",
      "Epoch 1/10 Loss: 5.942491071624755\n",
      "Epoch 2/10 Loss: 5.83808203125\n",
      "Epoch 3/10 Loss: 5.774402575378418\n",
      "Epoch 4/10 Loss: 5.769352248840332\n",
      "Epoch 5/10 Loss: 5.74943915802002\n",
      "Epoch 6/10 Loss: 5.750322311706543\n",
      "Epoch 7/10 Loss: 5.74098383392334\n",
      "Epoch 8/10 Loss: 5.7399361894226075\n",
      "Epoch 9/10 Loss: 5.728250364532471\n",
      "Epoch 10/10 Loss: 5.731992821960449\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:47:10.952281Z",
     "start_time": "2024-05-21T08:47:06.421054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate perplexity\n",
    "def perplexity(model, dl, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for X, y in dl:\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "    return np.exp(total_loss / len(dl))\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(torch.load(\"nplm_brown.pt\"))\n",
    "\n",
    "test_perplexity = perplexity(model, test_dl, criterion)\n",
    "val_perplexity = perplexity(model, val_dl, criterion)\n",
    "print(f\"Test Perplexity: {test_perplexity}\")\n",
    "print(f\"Validation Perplexity: {val_perplexity}\")"
   ],
   "id": "1a4dacb35e69ff73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 307.4282649635557\n",
      "Validation Perplexity: 287.0144941594557\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
